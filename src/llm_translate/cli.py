"""
å‘½ä»¤è¡Œå…¥å£
"""

import argparse
import json
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

from llm_translate.config import (
    API_KEY,
    EU_LANGUAGES,
    AVAILABLE_MODELS,
    DEFAULT_TARGET_LANGS,
    get_model_short_name,
)
from llm_translate.translator import (
    multi_translate,
    evaluate_translations,
    MultiTranslateResult,
)

console = Console()


def print_result(result: MultiTranslateResult):
    """æ‰“å°ç¿»è¯‘ç»“æœ"""
    console.print(Panel.fit(
        f"[bold]æºæ–‡æœ¬ ({result.source_lang})[/bold]\n{result.source_text}",
        border_style="blue"
    ))
    console.print()

    if not result.success:
        console.print(f"[red]ç¿»è¯‘å¤±è´¥: {result.error}[/red]")
        return

    table = Table(
        title=f"ç¿»è¯‘ç»“æœ (æ¨¡å‹: {result.model})",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold cyan"
    )
    table.add_column("è¯­è¨€", style="bold", width=15)
    table.add_column("ç¿»è¯‘ç»“æœ", width=60)

    for lang_code in sorted(result.translations.keys()):
        lang_name = EU_LANGUAGES.get(lang_code, lang_code)
        table.add_row(lang_name, result.translations[lang_code])

    console.print(table)
    console.print()
    console.print(
        f"[dim]å•æ¬¡ API è°ƒç”¨ | å»¶è¿Ÿ: {result.latency_ms:.0f}ms | "
        f"Tokens: {result.total_tokens} (è¾“å…¥: {result.prompt_tokens}, è¾“å‡º: {result.completion_tokens})[/dim]"
    )


def print_evaluation(eval_result, translation_model: str):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    table = Table(
        title="ç¿»è¯‘è´¨é‡è¯„åˆ† (è¯„ä¼°æ¨¡å‹: Opus 4.5)",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold magenta"
    )
    table.add_column("è¯­è¨€", style="bold", width=17)
    table.add_column("å‡†ç¡®æ€§", justify="center", width=6)
    table.add_column("æµç•…åº¦", justify="center", width=6)
    table.add_column("é£æ ¼", justify="center", width=6)
    table.add_column("ç»¼åˆåˆ†", justify="center", width=6)
    table.add_column("è¯„è¯­", width=40)

    total_overall = 0
    count = 0

    def score_color(s):
        if s >= 9:
            return "green"
        elif s >= 7:
            return "yellow"
        return "red"

    for lang_code in sorted(eval_result.scores.keys()):
        score = eval_result.scores[lang_code]
        lang_name = EU_LANGUAGES.get(lang_code, lang_code)
        comments = score.comments[:40] + "..." if len(score.comments) > 40 else score.comments

        table.add_row(
            lang_name,
            f"[{score_color(score.accuracy)}]{score.accuracy}[/]",
            f"[{score_color(score.fluency)}]{score.fluency}[/]",
            f"[{score_color(score.style)}]{score.style}[/]",
            f"[bold {score_color(score.overall)}]{score.overall:.1f}[/]",
            comments
        )
        total_overall += score.overall
        count += 1

    console.print(table)

    if count > 0:
        avg = total_overall / count
        color = "green" if avg >= 8 else "yellow" if avg >= 6 else "red"
        console.print(f"\n[bold]å¹³å‡ç»¼åˆåˆ†: [{color}]{avg:.2f}/10[/][/bold]")

    console.print(f"[dim]è¯„ä¼°è€—æ—¶: {eval_result.latency_ms:.0f}ms | Tokens: {eval_result.total_tokens}[/dim]")


def cmd_translate(args):
    """ç¿»è¯‘å‘½ä»¤"""
    if args.file:
        text = Path(args.file).read_text(encoding="utf-8").strip()
    elif args.text:
        text = args.text
    else:
        console.print("[red]é”™è¯¯: è¯·æä¾›è¦ç¿»è¯‘çš„æ–‡æœ¬[/red]")
        return 1

    if not API_KEY:
        console.print("[red]é”™è¯¯: æœªè®¾ç½® API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®[/red]")
        return 1

    console.print(Panel.fit("[bold blue]å¤šè¯­è¨€ç¿»è¯‘ - ä¸€æ¬¡ API è°ƒç”¨[/bold blue]", border_style="blue"))
    console.print(f"æ¨¡å‹: {args.model}")
    console.print(f"æºè¯­è¨€: {args.source}")
    console.print(f"ç›®æ ‡è¯­è¨€: {', '.join(args.targets)} ({len(args.targets)}ä¸ª)")
    if args.glossary:
        console.print(f"æœ¯è¯­è¡¨: {args.glossary}")
    console.print()

    console.print("[cyan]æ­£åœ¨ç¿»è¯‘...[/cyan]")
    result = multi_translate(
        text=text,
        source_lang=args.source,
        target_langs=args.targets,
        model=args.model,
        glossary=args.glossary,
    )

    print_result(result)

    if args.eval and result.success:
        console.print()
        console.print("[cyan]æ­£åœ¨ä½¿ç”¨ Opus 4.5 è¯„ä¼°ç¿»è¯‘è´¨é‡...[/cyan]")
        eval_result = evaluate_translations(
            source_text=text,
            translations=result.translations,
            source_lang=args.source,
        )
        console.print()
        print_evaluation(eval_result, args.model)

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        console.print(f"[green]ç»“æœå·²ä¿å­˜åˆ°: {args.output}[/green]")

    return 0 if result.success else 1


@dataclass
class SingleResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    text_type: str
    text: str
    success: bool
    latency_ms: float
    score: Optional[float]
    error: Optional[str] = None


def cmd_benchmark(args):
    """åŸºå‡†æµ‹è¯•å‘½ä»¤"""
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path(args.data)
    if not data_file.exists():
        console.print(f"[red]é”™è¯¯: æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}[/red]")
        return 1

    with open(data_file, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    titles = test_data.get("titles", [])
    descriptions = test_data.get("descriptions", [])
    all_texts = [(t, "title") for t in titles] + [(d, "description") for d in descriptions]

    models = args.models or AVAILABLE_MODELS
    target_langs = args.targets

    glossary = getattr(args, 'glossary', None)
    concurrency = getattr(args, 'concurrency', 1)

    console.print(f"\n[bold blue]{'=' * 60}[/bold blue]")
    console.print("[bold blue]ç”µå•†ç¿»è¯‘å…¨æ¨¡å‹åŸºå‡†æµ‹è¯•[/bold blue]")
    console.print(f"[bold blue]{'=' * 60}[/bold blue]")
    console.print(f"\næ¨¡å‹æ•°é‡: {len(models)}")
    console.print(f"æµ‹è¯•æ–‡æœ¬: {len(titles)} æ ‡é¢˜ + {len(descriptions)} æè¿°")
    console.print(f"ç›®æ ‡è¯­è¨€: {len(target_langs)} ä¸ª")
    console.print(f"å¹¶å‘åº¦: {concurrency} (æ¯æ¨¡å‹)")
    if glossary:
        console.print(f"æœ¯è¯­è¡¨: {glossary}")

    results = []
    lock = threading.Lock()

    def test_model(model: str) -> dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        model_short = get_model_short_name(model)
        model_results = [None] * len(all_texts)  # é¢„åˆ†é…ä¿æŒé¡ºåº
        start_time = time.time()
        completed_count = [0]  # ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

        def process_single(idx: int, text: str, text_type: str) -> None:
            """å¤„ç†å•ä¸ªæ–‡æœ¬"""
            try:
                result = multi_translate(
                    text=text,
                    source_lang="en",
                    target_langs=target_langs,
                    model=model,
                    glossary=glossary,
                )

                score = None
                if not args.no_eval and result.success:
                    eval_result = evaluate_translations(
                        source_text=text,
                        translations=result.translations,
                        source_lang="en",
                    )
                    if eval_result.scores:
                        score = sum(s.overall for s in eval_result.scores.values()) / len(eval_result.scores)

                model_results[idx] = SingleResult(
                    text_type=text_type,
                    text=text[:50] + "..." if len(text) > 50 else text,
                    success=result.success,
                    latency_ms=result.latency_ms,
                    score=score,
                )

                with lock:
                    completed_count[0] += 1
                    console.print(f"  [{model_short}] {completed_count[0]}/{len(all_texts)} å®Œæˆ" +
                                  (f", è¯„åˆ†: {score:.1f}" if score else ""))

            except Exception as e:
                model_results[idx] = SingleResult(
                    text_type=text_type,
                    text=text[:50] + "...",
                    success=False,
                    latency_ms=0,
                    score=None,
                    error=str(e),
                )
                with lock:
                    completed_count[0] += 1

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if concurrency > 1:
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []
                for i, (text, text_type) in enumerate(all_texts):
                    futures.append(executor.submit(process_single, i, text, text_type))
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for f in futures:
                    f.result()
        else:
            # ä¸²è¡Œå¤„ç†
            for i, (text, text_type) in enumerate(all_texts):
                process_single(i, text, text_type)

        total_time = time.time() - start_time
        # è¿‡æ»¤ None å€¼ï¼ˆå¹¶å‘æ—¶çš„å®‰å…¨æ£€æŸ¥ï¼‰
        valid_results = [r for r in model_results if r is not None]
        success_count = sum(1 for r in valid_results if r.success)
        title_scores = [r.score for r in valid_results if r.text_type == "title" and r.score]
        desc_scores = [r.score for r in valid_results if r.text_type == "description" and r.score]
        all_scores = [r.score for r in valid_results if r.score]
        latencies = [r.latency_ms for r in valid_results if r.success]

        return {
            "model": model,
            "model_short": model_short,
            "title_avg_score": sum(title_scores) / len(title_scores) if title_scores else None,
            "desc_avg_score": sum(desc_scores) / len(desc_scores) if desc_scores else None,
            "overall_avg_score": sum(all_scores) / len(all_scores) if all_scores else None,
            "avg_latency_ms": sum(latencies) / len(latencies) if latencies else 0,
            "success_rate": f"{success_count}/{len(valid_results)}",
            "total_time_s": total_time,
        }

    console.print(f"\n[bold cyan]å¼€å§‹å¹¶è¡Œæµ‹è¯• {len(models)} ä¸ªæ¨¡å‹[/bold cyan]\n")

    with ThreadPoolExecutor(max_workers=len(models)) as executor:
        futures = {executor.submit(test_model, m): m for m in models}
        for future in as_completed(futures):
            model = futures[future]
            try:
                result = future.result()
                results.append(result)
                score_str = f"è¯„åˆ† {result['overall_avg_score']:.2f}/10, " if result['overall_avg_score'] else ""
                console.print(
                    f"[green]âœ“ {result['model_short']} å®Œæˆ: "
                    f"{score_str}"
                    f"è€—æ—¶ {result['total_time_s']:.1f}s[/green]"
                )
            except Exception as e:
                console.print(f"[red]âœ— {get_model_short_name(model)} å¤±è´¥: {e}[/red]")

    # æ‰“å°ç»“æœè¡¨æ ¼
    results.sort(key=lambda x: x["overall_avg_score"] or 0, reverse=True)

    table = Table(
        title="\nç”µå•†ç¿»è¯‘å…¨æ¨¡å‹æµ‹è¯•ç»“æœ",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold magenta"
    )
    table.add_column("æ’å", justify="center", width=4)
    table.add_column("æ¨¡å‹", style="bold", width=22)
    table.add_column("æ ‡é¢˜è¯„åˆ†", justify="center", width=10)
    table.add_column("æè¿°è¯„åˆ†", justify="center", width=10)
    table.add_column("æ€»è¯„åˆ†", justify="center", width=10)
    table.add_column("å¹³å‡å»¶è¿Ÿ", justify="center", width=10)
    table.add_column("æˆåŠŸç‡", justify="center", width=8)

    for i, r in enumerate(results, 1):
        def score_fmt(s):
            if s is None:
                return "[dim]N/A[/dim]"
            color = "green" if s >= 9 else "yellow" if s >= 8 else "red"
            return f"[{color}]{s:.2f}[/]"

        rank = f"ğŸ†{i}" if i == 1 else f"  {i}"
        table.add_row(
            rank,
            r["model_short"],
            score_fmt(r["title_avg_score"]),
            score_fmt(r["desc_avg_score"]),
            f"[bold]{score_fmt(r['overall_avg_score'])}[/bold]",
            f"{r['avg_latency_ms']:.0f}ms",
            r["success_rate"],
        )

    console.print(table)

    # ä¿å­˜ç»“æœ
    test_time = time.strftime("%Y-%m-%d %H:%M:%S")
    output = {
        "test_time": test_time,
        "models_count": len(results),
        "results": results,
    }

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    if args.output:
        output_file = Path(args.output)
    else:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = Path(f"results/benchmark_{timestamp}.json")

    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    console.print(f"\n[green]ç»“æœå·²ä¿å­˜åˆ°: {output_file}[/green]")
    return 0


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        prog="llm-translate",
        description="LLM å¤šè¯­è¨€ç¿»è¯‘åŸºå‡†æµ‹è¯•å·¥å…·",
    )
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # translate å‘½ä»¤
    p_translate = subparsers.add_parser("translate", help="ç¿»è¯‘æ–‡æœ¬")
    p_translate.add_argument("text", nargs="?", help="è¦ç¿»è¯‘çš„æ–‡æœ¬")
    p_translate.add_argument("-f", "--file", help="ä»æ–‡ä»¶è¯»å–æ–‡æœ¬")
    p_translate.add_argument(
        "-t", "--targets",
        nargs="+",
        default=DEFAULT_TARGET_LANGS,
        help="ç›®æ ‡è¯­è¨€ä»£ç åˆ—è¡¨"
    )
    p_translate.add_argument("-s", "--source", default="en", help="æºè¯­è¨€ä»£ç ")
    p_translate.add_argument("-m", "--model", default="gemini-2.5-flash-lite", help="ä½¿ç”¨çš„æ¨¡å‹")
    p_translate.add_argument("-g", "--glossary", help="æœ¯è¯­è¡¨ (fashion_hard, fashion_core, fashion_full, ecommerce)")
    p_translate.add_argument("-o", "--output", help="ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶")
    p_translate.add_argument("-e", "--eval", action="store_true", help="ä½¿ç”¨ Opus 4.5 è¯„ä¼°è´¨é‡")
    p_translate.set_defaults(func=cmd_translate)

    # benchmark å‘½ä»¤
    p_benchmark = subparsers.add_parser("benchmark", help="è¿è¡ŒåŸºå‡†æµ‹è¯•")
    p_benchmark.add_argument(
        "-d", "--data",
        default="data/ecommerce.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶"
    )
    p_benchmark.add_argument(
        "-m", "--models",
        nargs="+",
        help="è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼‰"
    )
    p_benchmark.add_argument(
        "-t", "--targets",
        nargs="+",
        default=DEFAULT_TARGET_LANGS,
        help="ç›®æ ‡è¯­è¨€ä»£ç åˆ—è¡¨"
    )
    p_benchmark.add_argument("--no-eval", action="store_true", help="è·³è¿‡è´¨é‡è¯„ä¼°")
    p_benchmark.add_argument(
        "-c", "--concurrency",
        type=int,
        default=1,
        help="æ¯ä¸ªæ¨¡å‹çš„å¹¶å‘åº¦ (é»˜è®¤: 1ï¼Œå³ä¸²è¡Œ)"
    )
    p_benchmark.add_argument(
        "-g", "--glossary",
        help="æœ¯è¯­è¡¨ (fashion_hard, fashion_core, fashion_full, ecommerce)"
    )
    p_benchmark.add_argument(
        "-o", "--output",
        default=None,
        help="è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: results/benchmark_YYYYMMDD_HHMMSS.json)"
    )
    p_benchmark.set_defaults(func=cmd_benchmark)

    # models å‘½ä»¤
    p_models = subparsers.add_parser("models", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    def cmd_models(args):
        for m in AVAILABLE_MODELS:
            console.print(f"  {m}")
        return 0
    p_models.set_defaults(func=cmd_models)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
