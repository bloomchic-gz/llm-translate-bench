"""
å‘½ä»¤è¡Œå…¥å£
"""

import argparse
import json
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

from llm_translate.config import (
    API_KEY,
    EU_LANGUAGES,
    AVAILABLE_MODELS,
    DEFAULT_TARGET_LANGS,
    EVALUATOR_MODEL,
    get_model_short_name,
)
from llm_translate.translator import (
    multi_translate,
    evaluate_translations,
    MultiTranslateResult,
)

console = Console()


def print_result(result: MultiTranslateResult):
    """æ‰“å°ç¿»è¯‘ç»“æœ"""
    # æ˜¾ç¤ºæºæ–‡æœ¬
    source_display = "\n".join(f"{i+1}. {t}" for i, t in enumerate(result.source_texts))
    console.print(Panel.fit(
        f"[bold]æºæ–‡æœ¬ ({result.source_lang}) - {len(result.source_texts)} æ¡[/bold]\n{source_display}",
        border_style="blue"
    ))
    console.print()

    if not result.success:
        console.print(f"[red]ç¿»è¯‘å¤±è´¥: {result.error}[/red]")
        return

    # å¤šæ–‡æœ¬ï¼šæ¯ä¸ªæ–‡æœ¬ä¸€ä¸ªè¡¨æ ¼
    for idx, source_text in enumerate(result.source_texts):
        if len(result.source_texts) > 1:
            console.print(f"[bold cyan]#{idx+1}[/bold cyan] {source_text}")

        table = Table(
            title=f"ç¿»è¯‘ç»“æœ (æ¨¡å‹: {result.model})" if len(result.source_texts) == 1 else None,
            box=box.ROUNDED,
            show_header=True,
            header_style="bold cyan"
        )
        table.add_column("è¯­è¨€", style="bold", width=15)
        table.add_column("ç¿»è¯‘ç»“æœ", width=60)

        for lang_code in sorted(result.translations.keys()):
            lang_name = EU_LANGUAGES.get(lang_code, lang_code)
            trans_list = result.translations[lang_code]
            trans_text = trans_list[idx] if idx < len(trans_list) else ""
            table.add_row(lang_name, trans_text)

        console.print(table)
        console.print()

    console.print(
        f"[dim]å•æ¬¡ API è°ƒç”¨ | {len(result.source_texts)} æ¡æ–‡æœ¬ | å»¶è¿Ÿ: {result.latency_ms:.0f}ms | "
        f"Tokens: {result.total_tokens} (è¾“å…¥: {result.prompt_tokens}, è¾“å‡º: {result.completion_tokens})[/dim]"
    )


def print_evaluation(eval_result, translation_model: str, evaluator_model: str = None):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    eval_model_name = get_model_short_name(evaluator_model) if evaluator_model else "Opus 4.5"
    table = Table(
        title=f"ç¿»è¯‘è´¨é‡è¯„åˆ† (è¯„ä¼°æ¨¡å‹: {eval_model_name})",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold magenta"
    )
    table.add_column("è¯­è¨€", style="bold", width=20)
    table.add_column("åˆ†æ•°", justify="center", width=10)

    total_overall = 0
    count = 0

    def score_color(s):
        """æ ¹æ®100åˆ†åˆ¶åˆ†æ•°è¿”å›é¢œè‰²"""
        if s >= 85:
            return "green"
        elif s >= 70:
            return "yellow"
        return "red"

    for lang_code in sorted(eval_result.scores.keys()):
        score = eval_result.scores[lang_code]
        lang_name = EU_LANGUAGES.get(lang_code, lang_code)

        table.add_row(
            lang_name,
            f"[bold {score_color(score.overall)}]{score.overall:.0f}[/]",
        )
        total_overall += score.overall
        count += 1

    console.print(table)

    if count > 0:
        avg = total_overall / count
        color = "green" if avg >= 85 else "yellow" if avg >= 70 else "red"
        console.print(f"\n[bold]å¹³å‡åˆ†: [{color}]{avg:.1f}/100[/][/bold]")

    console.print(f"[dim]è¯„ä¼°è€—æ—¶: {eval_result.latency_ms:.0f}ms | Tokens: {eval_result.total_tokens}[/dim]")


def print_evaluation_multi(eval_result, translation_model: str, evaluator_model: str = None):
    """æ‰“å°å¤šæ–‡æœ¬è¯„ä¼°ç»“æœ"""
    eval_model_name = get_model_short_name(evaluator_model) if evaluator_model else "Opus 4.5"
    num_texts = len(eval_result.source_texts)

    def score_color(s):
        if s >= 85:
            return "green"
        elif s >= 70:
            return "yellow"
        return "red"

    # æ±‡æ€»è¡¨æ ¼
    table = Table(
        title=f"ç¿»è¯‘è´¨é‡è¯„åˆ† - {num_texts} æ¡æ–‡æœ¬ (è¯„ä¼°æ¨¡å‹: {eval_model_name})",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold magenta"
    )
    table.add_column("è¯­è¨€", style="bold", width=20)
    table.add_column("å¹³å‡åˆ†", justify="center", width=10)
    table.add_column("åˆ†æ•°åˆ†å¸ƒ", width=40)

    total_overall = 0
    count = 0

    for lang_code in sorted(eval_result.scores.keys()):
        score = eval_result.scores[lang_code]
        lang_name = EU_LANGUAGES.get(lang_code, lang_code)

        # åˆ†æ•°åˆ†å¸ƒ
        if score.individual_scores:
            scores_str = ", ".join(str(int(s)) for s in score.individual_scores[:10])
            if len(score.individual_scores) > 10:
                scores_str += f"... (+{len(score.individual_scores)-10})"
        else:
            scores_str = "-"

        table.add_row(
            lang_name,
            f"[bold {score_color(score.overall)}]{score.overall:.1f}[/]",
            f"[dim]{scores_str}[/dim]",
        )
        total_overall += score.overall
        count += 1

    console.print(table)

    if count > 0:
        avg = total_overall / count
        color = "green" if avg >= 85 else "yellow" if avg >= 70 else "red"
        console.print(f"\n[bold]æ€»å¹³å‡åˆ†: [{color}]{avg:.1f}/100[/][/bold]")

    console.print(f"[dim]è¯„ä¼°è€—æ—¶: {eval_result.latency_ms:.0f}ms | Tokens: {eval_result.total_tokens} | æ–‡æœ¬æ•°: {num_texts}[/dim]")


def cmd_translate(args):
    """ç¿»è¯‘å‘½ä»¤"""
    if args.file:
        text = Path(args.file).read_text(encoding="utf-8").strip()
        texts = [text]
    elif args.texts:
        texts = args.texts
    else:
        console.print("[red]é”™è¯¯: è¯·æä¾›è¦ç¿»è¯‘çš„æ–‡æœ¬[/red]")
        return 1

    if not API_KEY:
        console.print("[red]é”™è¯¯: æœªè®¾ç½® API_KEYï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®[/red]")
        return 1

    console.print(Panel.fit("[bold blue]å¤šè¯­è¨€ç¿»è¯‘ - ä¸€æ¬¡ API è°ƒç”¨[/bold blue]", border_style="blue"))
    console.print(f"æ¨¡å‹: {args.model}")
    console.print(f"æºè¯­è¨€: {args.source}")
    console.print(f"ç›®æ ‡è¯­è¨€: {', '.join(args.targets)} ({len(args.targets)}ä¸ª)")
    console.print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")
    if args.glossary:
        console.print(f"æœ¯è¯­è¡¨: {args.glossary}")
    console.print()

    console.print("[cyan]æ­£åœ¨ç¿»è¯‘...[/cyan]")
    result = multi_translate(
        texts=texts,
        source_lang=args.source,
        target_langs=args.targets,
        model=args.model,
        glossary=args.glossary,
        translate_prompt=args.translate_prompt,
    )

    print_result(result)

    if args.eval and result.success:
        console.print()
        eval_model_short = get_model_short_name(args.evaluator_model)
        console.print(f"[cyan]æ­£åœ¨ä½¿ç”¨ {eval_model_short} è¯„ä¼°ç¿»è¯‘è´¨é‡ ({len(texts)} æ¡æ–‡æœ¬)...[/cyan]")
        eval_result = evaluate_translations(
            source_texts=texts,
            translations=result.translations,
            source_lang=args.source,
            evaluator_model=args.evaluator_model,
            evaluate_prompt=args.evaluate_prompt,
        )
        console.print()
        print_evaluation_multi(eval_result, args.model, args.evaluator_model)

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        console.print(f"[green]ç»“æœå·²ä¿å­˜åˆ°: {args.output}[/green]")

    return 0 if result.success else 1


@dataclass
class SingleResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    text_type: str
    text: str
    success: bool
    latency_ms: float
    score: Optional[float]  # ç¬¬ä¸€ä¸ªè¯„ä¼°æ¨¡å‹çš„åˆ†æ•°ï¼ˆå…¼å®¹ï¼‰
    error: Optional[str] = None
    # è¯¦ç»†ç»“æœ
    translations: Optional[dict] = None  # å„è¯­è¨€ç¿»è¯‘ç»“æœ
    eval_scores: Optional[dict] = None   # å„è¯­è¨€è¯„ä¼°è¯¦æƒ…ï¼ˆç¬¬ä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼‰
    eval_latency_ms: Optional[float] = None  # è¯„ä¼°è€—æ—¶ï¼ˆç¬¬ä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼‰
    # å¤šè¯„ä¼°æ¨¡å‹æ”¯æŒ
    multi_eval: Optional[dict] = None  # {evaluator_model: {score, eval_scores, eval_latency_ms, tokens}}
    # ç¿»è¯‘ Token ç»Ÿè®¡
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None
    # è¯„ä¼° Token ç»Ÿè®¡ï¼ˆç¬¬ä¸€ä¸ªè¯„ä¼°æ¨¡å‹ï¼Œå…¼å®¹ï¼‰
    eval_prompt_tokens: Optional[int] = None
    eval_completion_tokens: Optional[int] = None
    eval_total_tokens: Optional[int] = None

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "text_type": self.text_type,
            "text": self.text,
            "success": self.success,
            "latency_ms": self.latency_ms,
            "score": self.score,
            "error": self.error,
            "translations": self.translations,
            "eval_scores": self.eval_scores,
            "eval_latency_ms": self.eval_latency_ms,
            "multi_eval": self.multi_eval,
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens,
            "eval_prompt_tokens": self.eval_prompt_tokens,
            "eval_completion_tokens": self.eval_completion_tokens,
            "eval_total_tokens": self.eval_total_tokens,
        }


def cmd_benchmark(args):
    """åŸºå‡†æµ‹è¯•å‘½ä»¤"""
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path(args.data)
    if not data_file.exists():
        console.print(f"[red]é”™è¯¯: æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}[/red]")
        return 1

    with open(data_file, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    titles = test_data.get("titles", [])
    descriptions = test_data.get("descriptions", [])
    all_texts = [(t, "title") for t in titles] + [(d, "description") for d in descriptions]

    models = args.models or AVAILABLE_MODELS
    target_langs = args.targets

    glossary = getattr(args, 'glossary', None)
    concurrency = getattr(args, 'concurrency', 1)
    translate_prompt = getattr(args, 'translate_prompt', None)
    evaluate_prompt = getattr(args, 'evaluate_prompt', None)
    evaluator_models = getattr(args, 'evaluator_model', [EVALUATOR_MODEL])
    if isinstance(evaluator_models, str):
        evaluator_models = [evaluator_models]

    console.print(f"\n[bold blue]{'=' * 60}[/bold blue]")
    console.print("[bold blue]ç”µå•†ç¿»è¯‘å…¨æ¨¡å‹åŸºå‡†æµ‹è¯•[/bold blue]")
    console.print(f"[bold blue]{'=' * 60}[/bold blue]")
    console.print(f"\næ¨¡å‹æ•°é‡: {len(models)}")
    console.print(f"æµ‹è¯•æ–‡æœ¬: {len(titles)} æ ‡é¢˜ + {len(descriptions)} æè¿°")
    console.print(f"ç›®æ ‡è¯­è¨€: {len(target_langs)} ä¸ª")
    console.print(f"å¹¶å‘åº¦: {concurrency} (æ¯æ¨¡å‹)")
    if not args.no_eval:
        eval_names = [get_model_short_name(m) for m in evaluator_models]
        console.print(f"è¯„ä¼°æ¨¡å‹: {', '.join(eval_names)} ({len(evaluator_models)}ä¸ª)")
    if glossary:
        console.print(f"æœ¯è¯­è¡¨: {glossary}")

    results = []
    lock = threading.Lock()

    def test_model(model: str) -> dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        model_short = get_model_short_name(model)
        model_results = [None] * len(all_texts)  # é¢„åˆ†é…ä¿æŒé¡ºåº
        start_time = time.time()
        completed_count = [0]  # ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹

        def process_single(idx: int, text: str, text_type: str) -> None:
            """å¤„ç†å•ä¸ªæ–‡æœ¬"""
            try:
                result = multi_translate(
                    texts=text,
                    source_lang="en",
                    target_langs=target_langs,
                    model=model,
                    glossary=glossary,
                    translate_prompt=translate_prompt,
                )

                score = None
                eval_scores = None
                eval_latency_ms = None
                eval_prompt_tokens = None
                eval_completion_tokens = None
                eval_total_tokens = None
                multi_eval = {}

                if not args.no_eval and result.success:
                    translations_dict = result.get_single_translations()

                    # å¯¹æ¯ä¸ªè¯„ä¼°æ¨¡å‹è¿›è¡Œè¯„ä¼°
                    for eval_idx, eval_model in enumerate(evaluator_models):
                        try:
                            eval_result = evaluate_translations(
                                source_texts=text,
                                translations=translations_dict,
                                source_lang="en",
                                evaluator_model=eval_model,
                                evaluate_prompt=evaluate_prompt,
                            )

                            if eval_result.scores:
                                eval_score = sum(s.overall for s in eval_result.scores.values()) / len(eval_result.scores)
                                eval_lang_scores = {
                                    lang: int(s.overall)
                                    for lang, s in eval_result.scores.items()
                                }

                                # å­˜å‚¨åˆ°å¤šè¯„ä¼°ç»“æœï¼ˆåŒ…å«tokenä¿¡æ¯ï¼‰
                                eval_model_short = get_model_short_name(eval_model)
                                multi_eval[eval_model_short] = {
                                    "score": eval_score,
                                    "eval_scores": eval_lang_scores,
                                    "eval_latency_ms": eval_result.latency_ms,
                                    "prompt_tokens": eval_result.prompt_tokens,
                                    "completion_tokens": eval_result.completion_tokens,
                                    "total_tokens": eval_result.total_tokens,
                                }

                                # ç¬¬ä¸€ä¸ªè¯„ä¼°æ¨¡å‹çš„ç»“æœä½œä¸ºé»˜è®¤ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                                if eval_idx == 0:
                                    score = eval_score
                                    eval_scores = eval_lang_scores
                                    eval_latency_ms = eval_result.latency_ms
                                    eval_prompt_tokens = eval_result.prompt_tokens
                                    eval_completion_tokens = eval_result.completion_tokens
                                    eval_total_tokens = eval_result.total_tokens
                        except Exception as eval_err:
                            eval_model_short = get_model_short_name(eval_model)
                            multi_eval[eval_model_short] = {
                                "score": None,
                                "error": str(eval_err),
                            }

                model_results[idx] = SingleResult(
                    text_type=text_type,
                    text=text,  # ä¿å­˜å®Œæ•´åŸæ–‡
                    success=result.success,
                    latency_ms=result.latency_ms,
                    score=score,
                    error=result.error if not result.success else None,
                    translations=result.get_single_translations() if result.success else None,
                    eval_scores=eval_scores,
                    eval_latency_ms=eval_latency_ms,
                    multi_eval=multi_eval if multi_eval else None,
                    prompt_tokens=result.prompt_tokens,
                    completion_tokens=result.completion_tokens,
                    total_tokens=result.total_tokens,
                    eval_prompt_tokens=eval_prompt_tokens,
                    eval_completion_tokens=eval_completion_tokens,
                    eval_total_tokens=eval_total_tokens,
                )

                with lock:
                    completed_count[0] += 1
                    console.print(f"  [{model_short}] {completed_count[0]}/{len(all_texts)} å®Œæˆ" +
                                  (f", è¯„åˆ†: {score:.0f}" if score else ""))

            except Exception as e:
                model_results[idx] = SingleResult(
                    text_type=text_type,
                    text=text,  # ä¿å­˜å®Œæ•´åŸæ–‡
                    success=False,
                    latency_ms=0,
                    score=None,
                    error=str(e),
                )
                with lock:
                    completed_count[0] += 1

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        if concurrency > 1:
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = []
                for i, (text, text_type) in enumerate(all_texts):
                    futures.append(executor.submit(process_single, i, text, text_type))
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for f in futures:
                    f.result()
        else:
            # ä¸²è¡Œå¤„ç†
            for i, (text, text_type) in enumerate(all_texts):
                process_single(i, text, text_type)

        total_time = time.time() - start_time
        # è¿‡æ»¤ None å€¼ï¼ˆå¹¶å‘æ—¶çš„å®‰å…¨æ£€æŸ¥ï¼‰
        valid_results = [r for r in model_results if r is not None]
        success_count = sum(1 for r in valid_results if r.success)
        title_scores = [r.score for r in valid_results if r.text_type == "title" and r.score]
        desc_scores = [r.score for r in valid_results if r.text_type == "description" and r.score]
        all_scores = [r.score for r in valid_results if r.score]
        latencies = [r.latency_ms for r in valid_results if r.success]

        # è®¡ç®—å„è¯„ä¼°æ¨¡å‹çš„å¹³å‡åˆ†
        multi_eval_scores = {}
        for eval_model_short in [get_model_short_name(m) for m in evaluator_models]:
            scores_for_eval = []
            for r in valid_results:
                if r.multi_eval and eval_model_short in r.multi_eval:
                    s = r.multi_eval[eval_model_short].get("score")
                    if s is not None:
                        scores_for_eval.append(s)
            if scores_for_eval:
                multi_eval_scores[eval_model_short] = sum(scores_for_eval) / len(scores_for_eval)

        return {
            "model": model,
            "model_short": model_short,
            "title_avg_score": sum(title_scores) / len(title_scores) if title_scores else None,
            "desc_avg_score": sum(desc_scores) / len(desc_scores) if desc_scores else None,
            "overall_avg_score": sum(all_scores) / len(all_scores) if all_scores else None,
            "avg_latency_ms": sum(latencies) / len(latencies) if latencies else 0,
            "success_rate": f"{success_count}/{len(valid_results)}",
            "total_time_s": total_time,
            # å¤šè¯„ä¼°æ¨¡å‹åˆ†æ•°
            "multi_eval_scores": multi_eval_scores,
            # è¯¦ç»†ç»“æœ
            "details": [r.to_dict() for r in valid_results],
        }

    console.print(f"\n[bold cyan]å¼€å§‹å¹¶è¡Œæµ‹è¯• {len(models)} ä¸ªæ¨¡å‹[/bold cyan]\n")

    with ThreadPoolExecutor(max_workers=len(models)) as executor:
        futures = {executor.submit(test_model, m): m for m in models}
        for future in as_completed(futures):
            model = futures[future]
            try:
                result = future.result()
                with lock:
                    results.append(result)
                score_str = f"è¯„åˆ† {result['overall_avg_score']:.1f}/100, " if result['overall_avg_score'] else ""
                console.print(
                    f"[green]âœ“ {result['model_short']} å®Œæˆ: "
                    f"{score_str}"
                    f"è€—æ—¶ {result['total_time_s']:.1f}s[/green]"
                )
            except Exception as e:
                console.print(f"[red]âœ— {get_model_short_name(model)} å¤±è´¥: {e}[/red]")

    # æ‰“å°ç»“æœè¡¨æ ¼
    results.sort(key=lambda x: x["overall_avg_score"] or 0, reverse=True)

    # è·å–è¯„ä¼°æ¨¡å‹çŸ­åç§°åˆ—è¡¨
    eval_model_names = [get_model_short_name(m) for m in evaluator_models]
    multi_eval_mode = len(evaluator_models) > 1

    table = Table(
        title="\nç”µå•†ç¿»è¯‘å…¨æ¨¡å‹æµ‹è¯•ç»“æœ",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold magenta"
    )
    table.add_column("æ’å", justify="center", width=4)
    table.add_column("æ¨¡å‹", style="bold", width=20)

    if multi_eval_mode:
        # å¤šè¯„ä¼°æ¨¡å‹ï¼šä¸ºæ¯ä¸ªè¯„ä¼°æ¨¡å‹æ·»åŠ ä¸€åˆ—
        for eval_name in eval_model_names:
            # ç®€åŒ–åç§°
            short_name = eval_name.replace("Gemini ", "G").replace("Claude ", "C").replace(" Flash", "F").replace(" Lite", "L").replace(" Pro", "P")
            table.add_column(short_name, justify="center", width=10)
    else:
        table.add_column("æ ‡é¢˜è¯„åˆ†", justify="center", width=10)
        table.add_column("æè¿°è¯„åˆ†", justify="center", width=10)
        table.add_column("æ€»è¯„åˆ†", justify="center", width=10)

    table.add_column("å¹³å‡å»¶è¿Ÿ", justify="center", width=10)
    table.add_column("æˆåŠŸç‡", justify="center", width=8)

    def score_fmt(s):
        if s is None:
            return "[dim]N/A[/dim]"
        color = "green" if s >= 90 else "yellow" if s >= 80 else "red"
        return f"[{color}]{s:.1f}[/]"

    for i, r in enumerate(results, 1):
        rank = f"ğŸ†{i}" if i == 1 else f"  {i}"

        if multi_eval_mode:
            # å¤šè¯„ä¼°æ¨¡å‹ï¼šæ˜¾ç¤ºæ¯ä¸ªè¯„ä¼°æ¨¡å‹çš„åˆ†æ•°
            row = [rank, r["model_short"]]
            for eval_name in eval_model_names:
                score = r.get("multi_eval_scores", {}).get(eval_name)
                row.append(score_fmt(score))
            row.append(f"{r['avg_latency_ms']:.0f}ms")
            row.append(r["success_rate"])
            table.add_row(*row)
        else:
            table.add_row(
                rank,
                r["model_short"],
                score_fmt(r["title_avg_score"]),
                score_fmt(r["desc_avg_score"]),
                f"[bold]{score_fmt(r['overall_avg_score'])}[/bold]",
                f"{r['avg_latency_ms']:.0f}ms",
                r["success_rate"],
            )

    console.print(table)

    # ä¿å­˜ç»“æœ
    test_time = time.strftime("%Y-%m-%d %H:%M:%S")
    # ä½¿ç”¨æ¯«ç§’çº§æ—¶é—´æˆ³é¿å…æ–‡ä»¶åå†²çª
    timestamp = f"{time.strftime('%Y%m%d_%H%M%S')}_{int(time.time() * 1000) % 1000:03d}"

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = Path(args.output)
        details_file = output_file.parent / "details" / output_file.name
    else:
        output_file = Path(f"results/benchmark_{timestamp}.json")
        details_file = Path(f"results/details/benchmark_{timestamp}.json")

    # æ±‡æ€»ç»“æœï¼ˆä¸å«è¯¦ç»†æ•°æ®ï¼‰
    summary_results = []
    for r in results:
        summary = {k: v for k, v in r.items() if k != "details"}
        summary_results.append(summary)

    summary_output = {
        "test_time": test_time,
        "config": {
            "data_file": str(data_file),
            "models_count": len(results),
            "titles_count": len(titles),
            "descriptions_count": len(descriptions),
            "target_langs": target_langs,
            "glossary": glossary,
            "concurrency": concurrency,
            "eval_enabled": not args.no_eval,
            "evaluator_models": evaluator_models if not args.no_eval else None,
        },
        "results": summary_results,
    }

    # è¯¦ç»†ç»“æœï¼ˆå«ç¿»è¯‘å’Œè¯„ä¼°æ˜ç»†ï¼‰
    details_output = {
        "test_time": test_time,
        "config": summary_output["config"],
        "results": results,  # åŒ…å« details
    }

    # ä¿å­˜æ±‡æ€»ç»“æœ
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(summary_output, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    details_file.parent.mkdir(parents=True, exist_ok=True)
    with open(details_file, "w", encoding="utf-8") as f:
        json.dump(details_output, f, ensure_ascii=False, indent=2)

    console.print(f"\n[green]æ±‡æ€»ç»“æœ: {output_file}[/green]")
    console.print(f"[green]è¯¦ç»†ç»“æœ: {details_file}[/green]")
    return 0


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        prog="llm-translate",
        description="LLM å¤šè¯­è¨€ç¿»è¯‘åŸºå‡†æµ‹è¯•å·¥å…·",
    )
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # translate å‘½ä»¤
    p_translate = subparsers.add_parser("translate", help="ç¿»è¯‘æ–‡æœ¬")
    p_translate.add_argument("texts", nargs="*", help="è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼ˆæ”¯æŒå¤šä¸ªï¼‰")
    p_translate.add_argument("-f", "--file", help="ä»æ–‡ä»¶è¯»å–æ–‡æœ¬")
    p_translate.add_argument(
        "-t", "--targets",
        nargs="+",
        default=DEFAULT_TARGET_LANGS,
        help="ç›®æ ‡è¯­è¨€ä»£ç åˆ—è¡¨"
    )
    p_translate.add_argument("-s", "--source", default="en", help="æºè¯­è¨€ä»£ç ")
    p_translate.add_argument("-m", "--model", default="gemini-2.5-flash-lite", help="ä½¿ç”¨çš„æ¨¡å‹")
    p_translate.add_argument("-g", "--glossary", help="æœ¯è¯­è¡¨ (fashion_hard, fashion_core, fashion_full, ecommerce)")
    p_translate.add_argument("-o", "--output", help="ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶")
    p_translate.add_argument("-e", "--eval", action="store_true", help="ä½¿ç”¨ Opus 4.5 è¯„ä¼°è´¨é‡")
    p_translate.add_argument("-tp", "--translate-prompt", help="ç¿»è¯‘æç¤ºè¯æ¨¡æ¿ (åç§°æˆ–æ–‡ä»¶è·¯å¾„)")
    p_translate.add_argument("-ep", "--evaluate-prompt", help="è¯„ä¼°æç¤ºè¯æ¨¡æ¿ (åç§°æˆ–æ–‡ä»¶è·¯å¾„)")
    p_translate.add_argument("-em", "--evaluator-model", default=EVALUATOR_MODEL, help="è¯„ä¼°æ¨¡å‹ (é»˜è®¤: Opus 4.5)")
    p_translate.set_defaults(func=cmd_translate)

    # benchmark å‘½ä»¤
    p_benchmark = subparsers.add_parser("benchmark", help="è¿è¡ŒåŸºå‡†æµ‹è¯•")
    p_benchmark.add_argument(
        "-d", "--data",
        default="data/ecommerce.json",
        help="æµ‹è¯•æ•°æ®æ–‡ä»¶"
    )
    p_benchmark.add_argument(
        "-m", "--models",
        nargs="+",
        help="è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé»˜è®¤æµ‹è¯•æ‰€æœ‰æ¨¡å‹ï¼‰"
    )
    p_benchmark.add_argument(
        "-t", "--targets",
        nargs="+",
        default=DEFAULT_TARGET_LANGS,
        help="ç›®æ ‡è¯­è¨€ä»£ç åˆ—è¡¨"
    )
    p_benchmark.add_argument("--no-eval", action="store_true", help="è·³è¿‡è´¨é‡è¯„ä¼°")
    p_benchmark.add_argument(
        "-c", "--concurrency",
        type=int,
        default=1,
        help="æ¯ä¸ªæ¨¡å‹çš„å¹¶å‘åº¦ (é»˜è®¤: 1ï¼Œå³ä¸²è¡Œ)"
    )
    p_benchmark.add_argument(
        "-g", "--glossary",
        help="æœ¯è¯­è¡¨ (fashion_hard, fashion_core, fashion_full, ecommerce)"
    )
    p_benchmark.add_argument(
        "-o", "--output",
        default=None,
        help="è¾“å‡ºæ–‡ä»¶ (é»˜è®¤: results/benchmark_YYYYMMDD_HHMMSS.json)"
    )
    p_benchmark.add_argument("-tp", "--translate-prompt", help="ç¿»è¯‘æç¤ºè¯æ¨¡æ¿ (åç§°æˆ–æ–‡ä»¶è·¯å¾„)")
    p_benchmark.add_argument("-ep", "--evaluate-prompt", help="è¯„ä¼°æç¤ºè¯æ¨¡æ¿ (åç§°æˆ–æ–‡ä»¶è·¯å¾„)")
    p_benchmark.add_argument("-em", "--evaluator-model", nargs="+", default=[EVALUATOR_MODEL], help="è¯„ä¼°æ¨¡å‹ï¼Œæ”¯æŒå¤šä¸ª (é»˜è®¤: Opus 4.5)")
    p_benchmark.set_defaults(func=cmd_benchmark)

    # models å‘½ä»¤
    p_models = subparsers.add_parser("models", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    def cmd_models(args):
        for m in AVAILABLE_MODELS:
            console.print(f"  {m}")
        return 0
    p_models.set_defaults(func=cmd_models)

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 0

    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
